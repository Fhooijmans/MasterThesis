{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocessing & EDA.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMGVb52T8foQ41yV4Z1ZHxD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fhooijmans/MasterThesis/blob/main/Preprocessing_%26_EDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hqdv3OUF27z"
      },
      "source": [
        "# import dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "df = pd.read_csv('release_10_23_2020.csv')\n",
        "print(df)\n",
        "\n",
        "\n",
        "######## Pre-Processing\n",
        "# Create df with sequence column\n",
        "# copy orignal dataset to new dataframe\n",
        "df_copy = df.copy()\n",
        "\n",
        "#replace nan values with pageview in product_action column\n",
        "df_copy[\"product_action\"].fillna(\"pageview\", inplace=True)\n",
        "df_copy[\"product_action\"].values\n",
        "\n",
        "\n",
        "#add column to df with event_symbols\n",
        "event_dict = {\"pageview\" : 1, \"detail\" : 2, \"add\" : 3, \"remove\" : 4, \"click\" : 5, \"purchase\" : 6}\n",
        "    #create list and fill with symbols via for-loop\n",
        "symbol_events = []\n",
        "for action in df_copy[\"product_action\"]:\n",
        "    symbol_events.append(event_dict[action]) \n",
        "    #add list to dataframe\n",
        "df_copy['symbol_event'] = symbol_events #list with symbols corresponding to product_action column added to df\n",
        "\n",
        "#get unique session id list from original dataset\n",
        "list_all_sessions = df_copy['session_id_hash'].copy() #series of all id occurences in dataset, same nr of rows as symbol_event list\n",
        "list_unique_sessions = list_all_sessions.unique() #list with unique ids\n",
        "\n",
        "#df with unique ids, to later append sequences to\n",
        "df_sequences = pd.DataFrame(list_unique_sessions.copy(), columns=['session_id'])\n",
        "df_sequences['sequence_events'] = \"\" #added empty sequence column for appending\n",
        "\n",
        "count_ids = 0 #index of unique id\n",
        "count_events = 0 #index to know at which event we are in the total symbol list, to not have to iterate over whole list for every session id\n",
        "for id_unique in list_unique_sessions:\n",
        "    list_events = [] # for each unique session id, an empty list is created to append the individual events to of that session\n",
        "    for id_all in list_all_sessions[count_events::]: \n",
        "# =============================================================================\n",
        "# Start of inner loop is based on event_counter, to prevent iterating over events that already have been processed\n",
        "# Count_events registers which event rows have been processed. Loop continues at session id row that made previous loop 'Break'.\n",
        "# This is less computational intensive because now this inner loop only iterates over the number of rows that match the sess_id + 1.\n",
        "# Using 'Continue' instead of 'Break' would make the inner loop also iterate over all remaining id_record rows (i.e. events) \n",
        "# after the final row that matched the currently checked sess_id, including ids that don't match the current checked sess_id, \n",
        "# which is is unnecessary compsumption of computational power.\n",
        "# =============================================================================\n",
        "        if id_unique == id_all:\n",
        "            list_events.append(symbol_events[count_events]) #symbol _event is separate list with all events in order\n",
        "        else:\n",
        "            break #When a row w. different id than sess_id is reached, loop breaks\n",
        "        count_events += 1 # counter + 1 for every event added to list_events\n",
        "    df_sequences.loc[count_ids, 'sequence_events'] = list_events #assigns sequence to corresponding session\n",
        "    count_ids += 1\n",
        "    \n",
        "#df1 = df.groupby('session_id_hash')['symbol'].apply(list).reset_index(name='sequence') \n",
        "## group by session and make list of symbols who belong to session\n",
        "\n",
        "#Export new df to CSV\n",
        "df_sequences.to_csv(r'all_sequences.csv', index = False)\n",
        "\n",
        "# Import df created above\n",
        "df_add_sequences = pd.read_csv('all_sequences.csv', converters={'sequence_events': eval})\n",
        "#pd.read_csv('all_sequences.csv', converters={'sequence_events': eval}) #keeps sequence column as list\n",
        "\n",
        "#convert sequence column from string to list -- sequences are strings when imported instead of lists\n",
        "df_add_sequences['sequence_events'] = df_add_sequences['sequence_events'].str.replace('[', \"\")\n",
        "df_add_sequences['sequence_events'] = df_add_sequences['sequence_events'].str.replace(']', \"\")\n",
        "#drop rows without 'add' event in sequence (i.e. '3')\n",
        "df_add_sequences = df_add_sequences[df_add_sequences['sequence_events'].str.contains('3')]\n",
        "df_add_sequences['sequence_events'] = df_add_sequences['sequence_events'].str.split(', ') # converts strings to lists\n",
        "df_add_sequences = df_add_sequences.reset_index(drop=True) #reset row index after dropping 'no add' rows\n",
        "\n",
        "### Trim purchase sequences to remove indication of purchase, and add P/A label column, P: Purchase, A: Abandonment (i.e. make purchase)\n",
        "    # add P/A label first\n",
        "df_add_sequences['target_label'] = \"\"\n",
        "    # Enumerate can be used to count row index when starting at first row and iterating over all rows\n",
        "for index, seq in enumerate(df_add_sequences['sequence_events']):\n",
        "    if '6' in seq:\n",
        "        df_add_sequences.loc[index, 'target_label'] = 'P'\n",
        "    else:\n",
        "        df_add_sequences.loc[index, 'target_label'] = 'A'\n",
        "\n",
        "    #Export df to CSV\n",
        "df_add_sequences.to_csv(r'add_sequences.csv', index = False)\n",
        "df_add_seq_trim = pd.read_csv('add_sequences.csv', converters={'sequence_events': eval}) # import updated new CSV\n",
        "    #trim sequences by removing purchase symbol (6), and subsequent symbols\n",
        "for seq in df_add_seq_trim['sequence_events']:\n",
        "    if '6' in seq:\n",
        "        index = seq.index('6')\n",
        "        length = len(seq)\n",
        "        seq[index:length] = \"\"\n",
        "\n",
        "\n",
        "### Add column w. sequence length, to give insight in lengths of all 'add' sequences till Purchase (P) OR Abandonment (A)\n",
        "    # empty column to fill w. seq lengths\n",
        "df_add_seq_trim['sequence_length'] = \"\"\n",
        "    # fill length column\n",
        "for index, seq in enumerate(df_add_seq_trim['sequence_events']):\n",
        "    seq_length = len(seq)\n",
        "    df_add_seq_trim.loc[index, 'sequence_length'] = seq_length\n",
        "    # Export df to CSV\n",
        "df_add_seq_trim.to_csv(r'length_sequences.csv', index = False) \n",
        "\n",
        "df_length = pd.read_csv('length_sequences.csv', converters={'sequence_events': eval}) # import updated new CSV\n",
        "\n",
        "# Removing too short/low sequences, this includes P seqs that were longer before trimming as done above\n",
        "    #copy of orignal df\n",
        "df_length = df_add_seq_trim.copy()\n",
        "    #drop rows with sequences of length <5\n",
        "df_length = df_length.drop(df_length[df_length.sequence_length < 5].index)\n",
        "    #drop rows with sequences of length >155\n",
        "df_length = df_length.drop(df_length[df_length.sequence_length > 155].index)\n",
        "\n",
        "df_length.to_csv(r'length_trim_sequences.csv', index = False)\n",
        "\n",
        "df_length = pd.read_csv('length_trim_sequences.csv', converters={'sequence_events': eval})\n",
        "\n",
        "######## EDA/Plots\n",
        "                                                #####Sequence length plots\n",
        "import matplotlib.pyplot as plt\n",
        "    #Length distribution after trim (4 < L < 156)\n",
        "df_abandon = df_length.drop(df_length[df_length.target_label == 'P'].index)\n",
        "df_purchase = df_length.drop(df_length[df_length.target_label == 'A'].index)\n",
        "fig = plt.figure(figsize=(8,4), dpi=150)\n",
        "plt.hist(df_abandon['sequence_length'], bins=75, histtype='stepfilled', density=True, alpha=0.5, log=True, color='red', label='A')\n",
        "plt.hist(df_purchase['sequence_length'], bins=75, histtype='stepfilled', density=True, alpha=0.5, log=True, color='limegreen', label='P')\n",
        "plt.ylim(0.0001, 1)\n",
        "plt.title('Length distribution add-to-cart sequences')\n",
        "plt.xlabel('Trajectory length L')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend()\n",
        "plt.grid(b=True)\n",
        "plt.show()\n",
        "fig.savefig('/Length_add.png', format='png')\n",
        "\n",
        "\n",
        "                                                ####dist. of events plot\n",
        "                                                \n",
        "df_abandon['pageview_count'] = [seq.count('1') for seq in df_abandon['sequence_events']]\n",
        "df_purchase['pageview_count'] = [seq.count('1') for seq in df_purchase['sequence_events']]\n",
        "\n",
        "df_abandon['detail_count'] = [seq.count('2') for seq in df_abandon['sequence_events']]\n",
        "df_purchase['detail_count'] = [seq.count('2') for seq in df_purchase['sequence_events']]\n",
        "\n",
        "df_abandon['add_count'] = [seq.count('3') for seq in df_abandon['sequence_events']]\n",
        "df_purchase['add_count'] = [seq.count('3') for seq in df_purchase['sequence_events']]\n",
        "\n",
        "df_abandon['remove_count'] = [seq.count('4') for seq in df_abandon['sequence_events']]\n",
        "df_purchase['remove_count'] = [seq.count('4') for seq in df_purchase['sequence_events']]\n",
        "\n",
        "df_abandon['click_count'] = [seq.count('5') for seq in df_abandon['sequence_events']]\n",
        "df_purchase['click_count'] = [seq.count('5') for seq in df_purchase['sequence_events']]\n",
        "\n",
        "\n",
        "r1 = ['Abandon']\n",
        "r2 = ['Purchase']\n",
        "width = 0.8\n",
        "# to percentage_abandon\n",
        "totals_abandon = [i+j+k+l+m for i,j,k,l,m in zip( [sum(df_abandon['pageview_count'])], [sum(df_abandon['detail_count'])], [sum(df_abandon['add_count'])], [sum(df_abandon['remove_count'])], [sum(df_abandon['click_count'])]) ]\n",
        "pageview_a = [i/j * 100 for i,j in zip([sum(df_abandon['pageview_count'])], totals_abandon)]\n",
        "detail_a = [i/j * 100 for i,j in zip([sum(df_abandon['detail_count'])], totals_abandon)]\n",
        "add_a = [i/j * 100 for i,j in zip([sum(df_abandon['add_count'])], totals_abandon)]\n",
        "remove_a = [i/j * 100 for i,j in zip([sum(df_abandon['remove_count'])], totals_abandon)]\n",
        "click_a = [i/j * 100 for i,j in zip([sum(df_abandon['click_count'])], totals_abandon)]\n",
        "\n",
        "# to percentage purchase\n",
        "totals_add = [i+j+k+l+m for i,j,k,l,m in zip( [sum(df_purchase['pageview_count'])], [sum(df_purchase['detail_count'])], [sum(df_purchase['add_count'])], [sum(df_purchase['remove_count'])], [sum(df_purchase['click_count'])]) ]\n",
        "pageview_add = [i/j * 100 for i,j in zip([sum(df_purchase['pageview_count'])], totals_add)]\n",
        "detail_add = [i/j * 100 for i,j in zip([sum(df_purchase['detail_count'])], totals_add)]\n",
        "add_add = [i/j * 100 for i,j in zip([sum(df_purchase['add_count'])], totals_add)]\n",
        "remove_add = [i/j * 100 for i,j in zip([sum(df_purchase['remove_count'])], totals_add)]\n",
        "click_add = [i/j * 100 for i,j in zip([sum(df_purchase['click_count'])], totals_add)]\n",
        "\n",
        "\n",
        "        #plot_abandon\n",
        "#pageview\n",
        "plt.bar(r1, pageview_a, color='#b5ffb9', edgecolor='white', label='pageview', width=width)\n",
        "#detail\n",
        "plt.bar(r1, detail_a, bottom=pageview_a, color='#f9bc86', edgecolor='white', label='detail', width=width)\n",
        "#add\n",
        "plt.bar(r1, add_a, bottom=[i+j for i,j in zip(pageview_a, detail_a)], color='#a3acff', edgecolor='white', label='add', width=width)    \n",
        "#remove\n",
        "plt.bar(r1, remove_a, bottom=[i+j+k for i,j,k in zip(pageview_a, detail_a, add_a)], color='#ffe873', edgecolor='white', label='remove', width=width)\n",
        "#click\n",
        "plt.bar(r1, click_a, bottom=[i+j+k+l for i,j,k,l in zip(pageview_a, detail_a, add_a, remove_a)], color='#ffc5bf', edgecolor='white', label='click', width=width)\n",
        "\n",
        "                          \n",
        "        #plot_purchase\n",
        "plt.bar(r2, pageview_add, color='#b5ffb9', edgecolor='white', width=width)\n",
        "#detail\n",
        "plt.bar(r2, detail_add, bottom=pageview_add, color='#f9bc86', edgecolor='white', width=width)\n",
        "#add\n",
        "plt.bar(r2, add_add, bottom=[i+j for i,j in zip(pageview_add, detail_add)], color='#a3acff', edgecolor='white', width=width)    \n",
        "#remove\n",
        "plt.bar(r2, remove_add, bottom=[i+j+k for i,j,k in zip(pageview_add, detail_add, add_add)], color='#ffe873', edgecolor='white', width=width)\n",
        "#click\n",
        "plt.bar(r2, click_add, bottom=[i+j+k+l for i,j,k,l in zip(pageview_add, detail_add, add_add, remove_add)], color='#ffc5bf', edgecolor='white', width=width)                           \n",
        "\n",
        "plt.title('Event distribution')                            \n",
        "plt.xlabel('Label')\n",
        "plt.ylabel('% of total events') \n",
        "plt.grid(b=True)      \n",
        "plt.legend(loc='lower center')    \n",
        "plt.show()\n",
        "\n",
        "                                                    #####mosaic plot\n",
        "#df_mosaic.to_csv(r'df_plot.csv', index = False)\n",
        "df_mosaic = pd.read_csv('df_plot.csv', converters={'sequence_events': eval})\n",
        "df_mosaic['remove_presence'] = ['present' if value != 0 else 'not present' for value in df_mosaic['remove_count']]\n",
        "\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "\n",
        "props={}\n",
        "props[('A','present')]={'facecolor':'xkcd:peach', 'edgecolor':'white'}\n",
        "props[('P','present')]={'facecolor':'xkcd:light blue', 'edgecolor':'white'}\n",
        "props[('A','not present')]={'facecolor':'xkcd:peach','edgecolor':'white'}\n",
        "props[('P','not present')]={'facecolor':'xkcd:light blue','edgecolor':'white'}\n",
        "\n",
        "labelizer = lambda k: {('A','present'):7645,('P','present'):3188,('A','not present'):22436,('P','not present'):3780}[k]\n",
        "mosaic(df_mosaic,['target_label','remove_presence'], properties=props, labelizer=labelizer, title='Label comparison by \"remove\"-event presence')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}